{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, recall_score, accuracy_score, precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "import optuna\n",
    "\n",
    "import interpret\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, LogisticRegression\n",
    "interpret.set_visualize_provider(interpret.provider.InlineProvider())\n",
    "\n",
    "from nam.data import NAMDataset\n",
    "from nam.config import defaults\n",
    "from nam.data import FoldedDataset\n",
    "from nam.models import NAM\n",
    "from nam.models import get_num_units\n",
    "from nam.trainer import LitNAM\n",
    "from nam.types import Config\n",
    "from nam.utils import parse_args\n",
    "from nam.utils import plot_mean_feature_importance\n",
    "from nam.utils import plot_nams\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- Kroll: income proejctions from Kroll (Kroll Net Cash Flow NCF) kroll data spalte DSCR_NCF, YearBuilt, orig (originator?), PropertyType, Deal (ID spalte), PropertyType (muss geschaut werden was abkürzungen bedeuten)\n",
    "- Green-Street: initial price paid for CMBS tranches\n",
    "- Green-Street: Risk Retention?\n",
    "- ExecuComp: Executive Compensation\n",
    "- SEC filings: Risk management practices\n",
    "- Boomberg: Information on fixed income underwrtigin 'league tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.read_excel('All_Data.xlsx', sheet_name=None)\n",
    "# all_data = pd.read_excel('All_Data.xlsx', sheet_name='CMBSMainDataFile.dta')\n",
    "with open('all_data.pickle', 'rb') as file:\n",
    "    all_data = pickle.load(file)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Non-Perorming: A dummy variable equal to one if a loan becomes non performing at any point from April 2020 to April 2021. A loan is non-perfomring if it has missed a payment, is in the process of foreclosure, is real estate owned, or has been placed into special servicing.\n",
    "cmbs_main = all_data['CMBSMainDataFile.dta']\n",
    "# cmbs_main = all_data\n",
    "cmbs_main[\n",
    "    [\n",
    "        'non_perf', \n",
    "        #'non_perf2',\n",
    "    ]\n",
    "].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distress: A variable equal to one if a loan appears on a servicer's watchlist or becomes non-perfomring from April 2020 to April 2021. From variables AprilWatch, MayWatch, JuneWatch, JulyWatch, Augwatch, Sepwatch, Oct20Watch, Nov20Watch, Dec20Watch, Jan21Watch, Feb21Watch, Mar21Watch, Apr21Watch\n",
    "#cmbs_main = all_data['CMBSMainDataFile.dta']\n",
    "distress = cmbs_main[['Distress']]\n",
    "cmbs_main[\n",
    "    [\n",
    "        'Distress', \n",
    "    ]\n",
    "].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(all_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal_characteristics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(cmbs_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "additional_columns = [\n",
    "    'Originator',\n",
    "    'ApprValUSD',\n",
    "    'CapRate',\n",
    "    'log_bal',\n",
    "    'LTV',\n",
    "    'DSCR',\n",
    "    'Size',\n",
    "    'RateType',\n",
    "    'buildingage',\n",
    "    'NOI',\n",
    "    'Occupancy',\n",
    "    'CutoffLTV',\n",
    "    'CutoffCpn',\n",
    "    'CutoffOcc',\n",
    "    'CutoffDSCR',\n",
    "    'CutoffNOIUSD',\n",
    "    'CutoffNCFUSD', \n",
    "    'fixed',\n",
    "] \n",
    "\n",
    "# age\n",
    "# Rate type\n",
    "# cmbs_main.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = cmbs_main[\n",
    "    [\n",
    "        'Deal', # ID Column\n",
    "        'OVER_w',\n",
    "        'past_over',\n",
    "        'high_overstatement2', # is 100% dependent on Over_w, if we predict this we get 100% accuracy\n",
    "        'Distress',\n",
    "        'non_perf'\n",
    "    ] + additional_columns\n",
    "]\n",
    "data = data.replace(' ', pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical data\n",
    "data['Originator'] = LabelEncoder().fit_transform(data['Originator'])\n",
    "data['RateType'] = LabelEncoder().fit_transform(data['RateType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_data = data[\n",
    "    data.notna().all(axis=1)\n",
    "]\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_data = clean_data.astype({'Occupancy': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_characteristics = all_data['OrigCharacteristics.dta']\n",
    "orig_characteristics_columns = [\n",
    "    #'Deal',\n",
    "    'type',\n",
    "    'CutoffLTV',\n",
    "    'CutoffDSCR',\n",
    "    'CutoffCpn',\n",
    "    'log_bal',\n",
    "    'fixed',\n",
    "    'buildingage',\n",
    "    'CutoffOcc',\n",
    "    'year_priced',\n",
    "    'quarter_type',\n",
    "    'AmortType',\n",
    "    # 'MSA',\n",
    "    'qy',\n",
    "    'Size',\n",
    "\n",
    "    'OVER_w',\n",
    "    'past_over',\n",
    "    'high_overstatement2', # is 100% dependent on Over_w, if we predict this we get 100% accuracy\n",
    "    'Distress',\n",
    "    #'non_perf'\n",
    "]\n",
    "orig_data = orig_characteristics[orig_characteristics_columns]\n",
    "# orig_characteristics['Originator'] = LabelEncoder().fit_transform(data['Originator'])\n",
    "# orig_characteristics['RateType'] = LabelEncoder().fit_transform(data['RateType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Distress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CutoffLTV</th>\n",
       "      <th>CutoffDSCR</th>\n",
       "      <th>CutoffCpn</th>\n",
       "      <th>log_bal</th>\n",
       "      <th>fixed</th>\n",
       "      <th>buildingage</th>\n",
       "      <th>CutoffOcc</th>\n",
       "      <th>year_priced</th>\n",
       "      <th>quarter_type</th>\n",
       "      <th>qy</th>\n",
       "      <th>...</th>\n",
       "      <th>AmortType_Fully Amort</th>\n",
       "      <th>AmortType_IO</th>\n",
       "      <th>AmortType_Partial IO</th>\n",
       "      <th>type_AGENCY-DUS</th>\n",
       "      <th>type_Agency</th>\n",
       "      <th>type_Conduit</th>\n",
       "      <th>type_Large Loan</th>\n",
       "      <th>type_Other</th>\n",
       "      <th>type_SASB</th>\n",
       "      <th>type_Small Bal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.664</td>\n",
       "      <td>1.340</td>\n",
       "      <td>4.990</td>\n",
       "      <td>16.518549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.954</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>32018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.316</td>\n",
       "      <td>4.140</td>\n",
       "      <td>14.440914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.949</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>22019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.587</td>\n",
       "      <td>1.266</td>\n",
       "      <td>4.410</td>\n",
       "      <td>16.871868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.806663</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.659</td>\n",
       "      <td>1.375</td>\n",
       "      <td>4.010</td>\n",
       "      <td>16.133181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>42014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.742</td>\n",
       "      <td>1.380</td>\n",
       "      <td>3.790</td>\n",
       "      <td>17.504391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>32015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40038</th>\n",
       "      <td>0.700</td>\n",
       "      <td>1.580</td>\n",
       "      <td>3.830</td>\n",
       "      <td>13.840203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.943</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>42012.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40040</th>\n",
       "      <td>0.743</td>\n",
       "      <td>1.570</td>\n",
       "      <td>5.910</td>\n",
       "      <td>15.800641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.983</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>22019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40041</th>\n",
       "      <td>0.642</td>\n",
       "      <td>1.318</td>\n",
       "      <td>3.948</td>\n",
       "      <td>17.448877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.984</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>42007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40042</th>\n",
       "      <td>0.732</td>\n",
       "      <td>1.900</td>\n",
       "      <td>4.838</td>\n",
       "      <td>16.491037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.985</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40044</th>\n",
       "      <td>0.531</td>\n",
       "      <td>1.215</td>\n",
       "      <td>4.580</td>\n",
       "      <td>17.425564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>42016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24469 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CutoffLTV  CutoffDSCR  CutoffCpn    log_bal  fixed  buildingage  \\\n",
       "1          0.664       1.340      4.990  16.518549    0.0     2.944439   \n",
       "4          0.750       1.316      4.140  14.440914    0.0     2.890372   \n",
       "6          0.587       1.266      4.410  16.871868    0.0     3.806663   \n",
       "7          0.659       1.375      4.010  16.133181    1.0     0.000000   \n",
       "8          0.742       1.380      3.790  17.504391    1.0     4.753590   \n",
       "...          ...         ...        ...        ...    ...          ...   \n",
       "40038      0.700       1.580      3.830  13.840203    1.0     2.890372   \n",
       "40040      0.743       1.570      5.910  15.800641    1.0     2.944439   \n",
       "40041      0.642       1.318      3.948  17.448877    1.0     1.609438   \n",
       "40042      0.732       1.900      4.838  16.491037    1.0     0.693147   \n",
       "40044      0.531       1.215      4.580  17.425564    0.0     3.465736   \n",
       "\n",
       "       CutoffOcc  year_priced  quarter_type       qy  ...  \\\n",
       "1          0.954       2015.0         192.0  32018.0  ...   \n",
       "4          0.949       2013.0         114.0  22019.0  ...   \n",
       "6          1.000       2019.0          47.0  32015.0  ...   \n",
       "7          0.962       2013.0          60.0  42014.0  ...   \n",
       "8          0.924       2018.0         127.0  32015.0  ...   \n",
       "...          ...          ...           ...      ...  ...   \n",
       "40038      0.943       2020.0         198.0  42012.0  ...   \n",
       "40040      0.983       2018.0         265.0  22019.0  ...   \n",
       "40041      0.984       2017.0         253.0  42007.0  ...   \n",
       "40042      0.985       2015.0          41.0  12015.0  ...   \n",
       "40044      1.000       2018.0         188.0  42016.0  ...   \n",
       "\n",
       "       AmortType_Fully Amort  AmortType_IO  AmortType_Partial IO  \\\n",
       "1                      False          True                 False   \n",
       "4                      False         False                  True   \n",
       "6                      False          True                 False   \n",
       "7                      False         False                 False   \n",
       "8                      False         False                 False   \n",
       "...                      ...           ...                   ...   \n",
       "40038                  False         False                  True   \n",
       "40040                  False          True                 False   \n",
       "40041                  False         False                 False   \n",
       "40042                  False         False                  True   \n",
       "40044                  False         False                 False   \n",
       "\n",
       "       type_AGENCY-DUS  type_Agency  type_Conduit  type_Large Loan  \\\n",
       "1                False         True         False            False   \n",
       "4                False         True         False            False   \n",
       "6                False         True         False            False   \n",
       "7                False         True         False            False   \n",
       "8                False         True         False            False   \n",
       "...                ...          ...           ...              ...   \n",
       "40038            False        False          True            False   \n",
       "40040            False        False          True            False   \n",
       "40041            False        False          True            False   \n",
       "40042            False         True         False            False   \n",
       "40044            False         True         False            False   \n",
       "\n",
       "       type_Other  type_SASB  type_Small Bal  \n",
       "1           False      False           False  \n",
       "4           False      False           False  \n",
       "6           False      False           False  \n",
       "7           False      False           False  \n",
       "8           False      False           False  \n",
       "...           ...        ...             ...  \n",
       "40038       False      False           False  \n",
       "40040       False      False           False  \n",
       "40041       False      False           False  \n",
       "40042       False      False           False  \n",
       "40044       False      False           False  \n",
       "\n",
       "[24469 rows x 26 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data_with_dummies = pd.get_dummies(\n",
    "    orig_data,\n",
    "    columns=[\n",
    "        'AmortType',\n",
    "        # 'MSA',\n",
    "        'type'\n",
    "    ]\n",
    ")\n",
    "clean_data = orig_data_with_dummies[\n",
    "    orig_data_with_dummies.notna().all(axis=1)\n",
    "]\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/tf6fbkqx6mv5m13cy7d765fw0000gn/T/ipykernel_8654/3247692996.py:3: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_cols = [col for col, dtype in clean_data.dtypes.items() if dtype == bool]\n",
    "for dummy_col in dummy_cols:\n",
    "    clean_data[dummy_col] = clean_data[dummy_col].map({True: 1, False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6110375827194406"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of clean data from whole dataset\n",
    "len(clean_data) / len(orig_data_with_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean_data[target_col].astype('U32')\n",
    "X = clean_data.drop(columns=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CutoffLTV</th>\n",
       "      <th>CutoffDSCR</th>\n",
       "      <th>CutoffCpn</th>\n",
       "      <th>log_bal</th>\n",
       "      <th>fixed</th>\n",
       "      <th>buildingage</th>\n",
       "      <th>CutoffOcc</th>\n",
       "      <th>year_priced</th>\n",
       "      <th>quarter_type</th>\n",
       "      <th>qy</th>\n",
       "      <th>...</th>\n",
       "      <th>AmortType_Fully Amort</th>\n",
       "      <th>AmortType_IO</th>\n",
       "      <th>AmortType_Partial IO</th>\n",
       "      <th>type_AGENCY-DUS</th>\n",
       "      <th>type_Agency</th>\n",
       "      <th>type_Conduit</th>\n",
       "      <th>type_Large Loan</th>\n",
       "      <th>type_Other</th>\n",
       "      <th>type_SASB</th>\n",
       "      <th>type_Small Bal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.664</td>\n",
       "      <td>1.340</td>\n",
       "      <td>4.990</td>\n",
       "      <td>16.518549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.954</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>32018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.316</td>\n",
       "      <td>4.140</td>\n",
       "      <td>14.440914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.949</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>22019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.587</td>\n",
       "      <td>1.266</td>\n",
       "      <td>4.410</td>\n",
       "      <td>16.871868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.806663</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.659</td>\n",
       "      <td>1.375</td>\n",
       "      <td>4.010</td>\n",
       "      <td>16.133181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>42014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.742</td>\n",
       "      <td>1.380</td>\n",
       "      <td>3.790</td>\n",
       "      <td>17.504391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>32015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40038</th>\n",
       "      <td>0.700</td>\n",
       "      <td>1.580</td>\n",
       "      <td>3.830</td>\n",
       "      <td>13.840203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.943</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>42012.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40040</th>\n",
       "      <td>0.743</td>\n",
       "      <td>1.570</td>\n",
       "      <td>5.910</td>\n",
       "      <td>15.800641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.983</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>22019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40041</th>\n",
       "      <td>0.642</td>\n",
       "      <td>1.318</td>\n",
       "      <td>3.948</td>\n",
       "      <td>17.448877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.984</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>42007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40042</th>\n",
       "      <td>0.732</td>\n",
       "      <td>1.900</td>\n",
       "      <td>4.838</td>\n",
       "      <td>16.491037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.985</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40044</th>\n",
       "      <td>0.531</td>\n",
       "      <td>1.215</td>\n",
       "      <td>4.580</td>\n",
       "      <td>17.425564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>42016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24469 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CutoffLTV  CutoffDSCR  CutoffCpn    log_bal  fixed  buildingage  \\\n",
       "1          0.664       1.340      4.990  16.518549    0.0     2.944439   \n",
       "4          0.750       1.316      4.140  14.440914    0.0     2.890372   \n",
       "6          0.587       1.266      4.410  16.871868    0.0     3.806663   \n",
       "7          0.659       1.375      4.010  16.133181    1.0     0.000000   \n",
       "8          0.742       1.380      3.790  17.504391    1.0     4.753590   \n",
       "...          ...         ...        ...        ...    ...          ...   \n",
       "40038      0.700       1.580      3.830  13.840203    1.0     2.890372   \n",
       "40040      0.743       1.570      5.910  15.800641    1.0     2.944439   \n",
       "40041      0.642       1.318      3.948  17.448877    1.0     1.609438   \n",
       "40042      0.732       1.900      4.838  16.491037    1.0     0.693147   \n",
       "40044      0.531       1.215      4.580  17.425564    0.0     3.465736   \n",
       "\n",
       "       CutoffOcc  year_priced  quarter_type       qy  ...  \\\n",
       "1          0.954       2015.0         192.0  32018.0  ...   \n",
       "4          0.949       2013.0         114.0  22019.0  ...   \n",
       "6          1.000       2019.0          47.0  32015.0  ...   \n",
       "7          0.962       2013.0          60.0  42014.0  ...   \n",
       "8          0.924       2018.0         127.0  32015.0  ...   \n",
       "...          ...          ...           ...      ...  ...   \n",
       "40038      0.943       2020.0         198.0  42012.0  ...   \n",
       "40040      0.983       2018.0         265.0  22019.0  ...   \n",
       "40041      0.984       2017.0         253.0  42007.0  ...   \n",
       "40042      0.985       2015.0          41.0  12015.0  ...   \n",
       "40044      1.000       2018.0         188.0  42016.0  ...   \n",
       "\n",
       "       AmortType_Fully Amort  AmortType_IO  AmortType_Partial IO  \\\n",
       "1                          0             1                     0   \n",
       "4                          0             0                     1   \n",
       "6                          0             1                     0   \n",
       "7                          0             0                     0   \n",
       "8                          0             0                     0   \n",
       "...                      ...           ...                   ...   \n",
       "40038                      0             0                     1   \n",
       "40040                      0             1                     0   \n",
       "40041                      0             0                     0   \n",
       "40042                      0             0                     1   \n",
       "40044                      0             0                     0   \n",
       "\n",
       "       type_AGENCY-DUS  type_Agency  type_Conduit  type_Large Loan  \\\n",
       "1                    0            1             0                0   \n",
       "4                    0            1             0                0   \n",
       "6                    0            1             0                0   \n",
       "7                    0            1             0                0   \n",
       "8                    0            1             0                0   \n",
       "...                ...          ...           ...              ...   \n",
       "40038                0            0             1                0   \n",
       "40040                0            0             1                0   \n",
       "40041                0            0             1                0   \n",
       "40042                0            1             0                0   \n",
       "40044                0            1             0                0   \n",
       "\n",
       "       type_Other  type_SASB  type_Small Bal  \n",
       "1               0          0               0  \n",
       "4               0          0               0  \n",
       "6               0          0               0  \n",
       "7               0          0               0  \n",
       "8               0          0               0  \n",
       "...           ...        ...             ...  \n",
       "40038           0          0               0  \n",
       "40040           0          0               0  \n",
       "40041           0          0               0  \n",
       "40042           0          0               0  \n",
       "40044           0          0               0  \n",
       "\n",
       "[24469 rows x 25 columns]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, X_leave_out, y, y_leave_out = train_test_split(input_data, target, stratify=target, random_state=1, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_oversampled, y_train_oversampled = SMOTE().fit_resample(X_train, y_train)\n",
    "# X_train_oversampled.hist()\n",
    "# X_train_oversampled['CapRate'].plot()\n",
    "# y_train_oversampled.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24469"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(y)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBM (Explainable Boosting Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebm_model_undersample = make_pipeline(\n",
    "#         RandomUnderSampler(random_state=42),\n",
    "#         ExplainableBoostingClassifier(random_state=42)\n",
    "# )\n",
    "# ebm_model_cv = cross_validate(\n",
    "#     ebm_model_undersample, X, y, scoring='roc_auc',\n",
    "#     return_train_score=True, return_estimator=True,\n",
    "#     n_jobs=-1 \n",
    "# )\n",
    "# print(\n",
    "#     f\"ROC AUC mean +/- std. dev.: \"\n",
    "#     f\"{ebm_model_cv['test_score'].mean():.3f} +/- \"\n",
    "#     f\"{ebm_model_cv['test_score'].std():.3f}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-05 18:46:23,936] A new study created in memory with name: EBM_study\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/distributions.py:685: UserWarning:\n",
      "\n",
      "The distribution is specified by [0.0009, 0.0015] and step=0.0004, but the range is not divisible by `step`. It will be replaced by [0.0009, 0.0013].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in fold: 0\n",
      "Oversampled data\n",
      "Starting to fit data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-09-05 18:46:35,698] Trial 0 failed with parameters: {'learning_rate': 0.0013} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/pr/tf6fbkqx6mv5m13cy7d765fw0000gn/T/ipykernel_8654/4098240991.py\", line 44, in objective\n",
      "    clf.fit(X_train_oversampled, y_train_oversampled)\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/interpret/glassbox/_ebm/_ebm.py\", line 894, in fit\n",
      "    results = provider.parallel(boost, parallel_args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/interpret/provider/_compute.py\", line 19, in parallel\n",
      "    results = Parallel(n_jobs=self.n_jobs)(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/Users/janik/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1707, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2023-09-05 18:46:35,705] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[433], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m study_ebm \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEBM_study\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m objective_edm \u001b[39m=\u001b[39m partial(objective, clf_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEBM\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[0;32m---> 59\u001b[0m study_ebm\u001b[39m.\u001b[39;49moptimize(objective_edm, n_trials\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     61\u001b[0m study_LR \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogisticRegression_study\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m objective_LR \u001b[39m=\u001b[39m partial(objective, clf_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogisticRegression\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     _optimize(\n\u001b[1;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[433], line 44\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, clf_type, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting to fit data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 44\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_oversampled, y_train_oversampled)\n\u001b[1;32m     45\u001b[0m total_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinished fitting data, took \u001b[39m\u001b[39m{\u001b[39;00mtotal_time\u001b[39m}\u001b[39;00m\u001b[39m sec.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/interpret/glassbox/_ebm/_ebm.py:894\u001b[0m, in \u001b[0;36mEBMModel.fit\u001b[0;34m(self, X, y, sample_weight, bags, init_score)\u001b[0m\n\u001b[1;32m    865\u001b[0m         init_score_local \u001b[39m=\u001b[39m init_score_local[bag \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    867\u001b[0m     parallel_args\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    868\u001b[0m         (\n\u001b[1;32m    869\u001b[0m             dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m         )\n\u001b[1;32m    892\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m results \u001b[39m=\u001b[39m provider\u001b[39m.\u001b[39;49mparallel(boost, parallel_args)\n\u001b[1;32m    896\u001b[0m \u001b[39m# let python reclaim the dataset memory via reference counting\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[39mdel\u001b[39;00m parallel_args  \u001b[39m# parallel_args holds references to dataset, so must be deleted\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/interpret/provider/_compute.py:19\u001b[0m, in \u001b[0;36mJobLibProvider.parallel\u001b[0;34m(self, compute_fn, compute_args_iter)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel\u001b[39m(\u001b[39mself\u001b[39m, compute_fn, compute_args_iter):\n\u001b[0;32m---> 19\u001b[0m     results \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m     20\u001b[0m         delayed(compute_fn)(\u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;49;00m args \u001b[39min\u001b[39;49;00m compute_args_iter\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Master/KIT/Semester 4/Advanced Machine Learning Projekt/.venv/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_res_models = {}\n",
    "# objective for optuna hyperparameter search\n",
    "def objective(trial, clf_type, X, y):\n",
    "    res = {\n",
    "        'scores': [],\n",
    "        'models': [],\n",
    "    }\n",
    "    if clf_type == 'EBM':\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.0009, 0.0015, step=0.0004)\n",
    "        smoothing_rounds = trial.suggest_trial('smoothing_rounds',low=0, high=4, step=2), # default is 0\n",
    "\n",
    "        model_name = \"EBM\"\n",
    "        clf = ExplainableBoostingClassifier(\n",
    "            random_state=42,\n",
    "            learning_rate=learning_rate,\n",
    "            smoothing_rounds=smoothing_rounds\n",
    "        )\n",
    "    \n",
    "    if clf_type == 'LogisticRegression':\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"elasticnet\", \"l2\", 'l1'])\n",
    "        C = trial.suggest_float(\"C\", 0.001, 2, step=0.5)\n",
    "        class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "        solver = trial.suggest_categorical('solver', ['newton-cholesky', 'lbfgs'])\n",
    "\n",
    "        model_name = \"LogisticRegression\"\n",
    "        clf = LogisticRegression(\n",
    "            random_state=42,\n",
    "            penalty=penalty,\n",
    "            C=C,\n",
    "            class_weight=class_weight,\n",
    "            solver=solver\n",
    "        )\n",
    "\n",
    "    # Use stratified K Fold instead of normal k fold to preserve class imbalance\n",
    "    strat_kfold = StratifiedKFold(n_splits=2)\n",
    "    # for train_idx, test_idx in tqdm(strat_kfold.split(X, y), total=strat_kfold.get_n_splits(), desc='K fold'):\n",
    "    for fold, (train_idx, test_idx) in enumerate(strat_kfold.split(X, y)):\n",
    "        print(f'Currently in fold: {fold}')\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "\n",
    "        X_test = X.iloc[test_idx, :]\n",
    "        y_test = y.iloc[test_idx]\n",
    "        X_train_oversampled, y_train_oversampled = SMOTE().fit_resample(X_train, y_train)\n",
    "        print('Oversampled data')\n",
    "        print('Starting to fit data')\n",
    "        start_time = time.perf_counter()\n",
    "        clf.fit(X_train_oversampled, y_train_oversampled)\n",
    "        total_time = time.perf_counter() - start_time\n",
    "        print(f'Finished fitting data, took {total_time} sec.')\n",
    "\n",
    "        auc_roc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        res['scores'].append(auc_roc)\n",
    "        res['models'].append(clf)\n",
    "\n",
    "    trial.set_user_attr(key='scores_and_models', value=res)\n",
    "    global_res_models.update({model_name: res})\n",
    "    return np.mean(auc_roc)\n",
    "\n",
    "# current best 0.54\n",
    "study_ebm = optuna.create_study(direction='maximize', study_name='EBM_study')\n",
    "objective_edm = partial(objective, clf_type='EBM', X=X, y=y)\n",
    "study_ebm.optimize(objective_edm, n_trials=1, show_progress_bar=True)\n",
    "# no current best\n",
    "study_LR = optuna.create_study(direction='maximize', study_name='LogisticRegression_study')\n",
    "objective_LR = partial(objective, clf_type='LogisticRegression', X=X, y=y)\n",
    "study_LR.optimize(objective_LR, n_trials=1, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "\n",
    "set_visualize_provider(InlineProvider())\n",
    "show(ebm.explain_global())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm.explain_global())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true=target.values, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=target.values, y_pred=y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.show(ROC(ebm_cv, X.columns).explain_perf(X, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class imbalance\n",
    "n_positives = len(y_train[y_train == '1.0'])\n",
    "n_negatives = len(y_train[y_train == '0.0'])\n",
    "ratio = n_positives / (n_positives + n_negatives)\n",
    "print(f'Ratio of samples is: {1-ratio}')\n",
    "\n",
    "\n",
    "# Calculate Scores\n",
    "acc_score = accuracy_score(target, y_pred)\n",
    "auc_roc = roc_auc_score(target, ebm_cv.predict_proba(X)[:, 1])\n",
    "print(f'Accuracy score: {acc_score}')\n",
    "print(f'ROC AUC score: {auc_roc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebm_model_smote = make_pipeline(\n",
    "#         SMOTE(random_state=42),\n",
    "#         ExplainableBoostingClassifier(random_state=42)\n",
    "# )\n",
    "# ebm_model_cv_smote = cross_validate(\n",
    "#     ebm_model_smote, X, y, scoring='roc_auc',\n",
    "#     return_train_score=True, return_estimator=True,\n",
    "#     n_jobs=-1 \n",
    "# )\n",
    "# print(\n",
    "#     f\"ROC AUC mean +/- std. dev.: \"\n",
    "#     f\"{ebm_model_cv_smote['test_score'].mean():.3f} +/- \"\n",
    "#     f\"{ebm_model_cv_smote['test_score'].std():.3f}\"\n",
    "# )\n",
    "# ebm_model_cv_smote\n",
    "# ebm_model_cv\n",
    "# ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "# # study = optuna.create_study(study_name='AML_EBM_study')\n",
    "# param_distributions = {\n",
    "#     'learning_rate': optuna.distributions.FloatDistribution(low=0.0009, high=0.0015, step=0.0004), # default is 0.001, best trial 0.0009\n",
    "#     # 'greediness': optuna.distributions.FloatDistribution(low=0.0, high=0.01, step=0.001), # default is 0\n",
    "#     # 'outer_bags': optuna.distributions.IntDistribution(low=5, high=10, step=1), # default is 8\n",
    "#     # 'inner_bags': optuna.distributions.IntDistribution(low=0, high=3), # default is 0\n",
    "#     # 'max_rounds': optuna.distributions.IntDistribution(low=4500, high=5500, step=500), # default is 5000\n",
    "#     'smoothing_rounds': optuna.distributions.IntDistribution(low=0, high=4, step=2), # default is 0\n",
    "#     # 'early_stopping_rounds': optuna.distributions.IntDistribution(low=40, high=60, step=15), # default is 50\n",
    "# }\n",
    "# ebm_cv = optuna.integration.OptunaSearchCV(\n",
    "#     ebm_model_undersample,\n",
    "#     param_distributions,\n",
    "#     n_trials=5,\n",
    "#     cv=4, # search with 3 fold CV\n",
    "#     # study=study\n",
    "# ) \n",
    "\n",
    "# ebm_cv.fit(X, y)\n",
    "# joblib.dump(ebm_cv, 'models/ebm_optuna_learning_rate.joblib')\n",
    "# y_pred = ebm_cv.predict(X)\n",
    "# y_pred\n",
    "# np.unique(y_pred)\n",
    "# len(y_pred[y_pred == 0])/ len(y_pred[y_pred == 1])\n",
    "# plt.hist(y_pred)\n",
    "# ebm_model_smote = make_pipeline(\n",
    "#         SMOTE(random_state=42),\n",
    "#         ExplainableBoostingClassifier(random_state=42)\n",
    "# )\n",
    "# ebm_model_cv_smote = cross_validate(\n",
    "#     ebm_model_smote, X, y, scoring='roc_auc',\n",
    "#     return_train_score=True, return_estimator=True,\n",
    "#     n_jobs=-1 \n",
    "# )\n",
    "# print(\n",
    "#     f\"ROC AUC mean +/- std. dev.: \"\n",
    "#     f\"{ebm_model_cv_smote['test_score'].mean():.3f} +/- \"\n",
    "#     f\"{ebm_model_cv_smote['test_score'].std():.3f}\"\n",
    "# )\n",
    "# ebm_model_cv_smote\n",
    "# ebm_model_cv\n",
    "# ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "# # study = optuna.create_study(study_name='AML_EBM_study')\n",
    "# param_distributions = {\n",
    "#     'learning_rate': optuna.distributions.FloatDistribution(low=0.0009, high=0.0015, step=0.0004), # default is 0.001, best trial 0.0009\n",
    "#     # 'greediness': optuna.distributions.FloatDistribution(low=0.0, high=0.01, step=0.001), # default is 0\n",
    "#     # 'outer_bags': optuna.distributions.IntDistribution(low=5, high=10, step=1), # default is 8\n",
    "#     # 'inner_bags': optuna.distributions.IntDistribution(low=0, high=3), # default is 0\n",
    "#     # 'max_rounds': optuna.distributions.IntDistribution(low=4500, high=5500, step=500), # default is 5000\n",
    "#     'smoothing_rounds': optuna.distributions.IntDistribution(low=0, high=4, step=2), # default is 0\n",
    "#     # 'early_stopping_rounds': optuna.distributions.IntDistribution(low=40, high=60, step=15), # default is 50\n",
    "# }\n",
    "# ebm_cv = optuna.integration.OptunaSearchCV(\n",
    "#     ebm_model_undersample,\n",
    "#     param_distributions,\n",
    "#     n_trials=5,\n",
    "#     cv=4, # search with 3 fold CV\n",
    "#     # study=study\n",
    "# ) \n",
    "\n",
    "# ebm_cv.fit(X, y)\n",
    "# joblib.dump(ebm_cv, 'models/ebm_optuna_learning_rate.joblib')\n",
    "# y_pred = ebm_cv.predict(X)\n",
    "# y_pred\n",
    "# np.unique(y_pred)\n",
    "# len(y_pred[y_pred == 0])/ len(y_pred[y_pred == 1])\n",
    "# plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitc Regression or XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0,2, (500,4))\n",
    "clean_data = pd.DataFrame(x, columns= ['a', 'b', 'c', 'd'])\n",
    "target_col = 'a'\n",
    "clean_data\n",
    "config\n",
    "# Neural Additive Model\n",
    "config = defaults()\n",
    "feature_cols = [col for col in clean_data.columns if col != target_col]\n",
    "nam_dataset = NAMDataset(\n",
    "    config,\n",
    "    data_path=clean_data,\n",
    "    features_columns=feature_cols,\n",
    "    targets_column=target_col,\n",
    ")\n",
    "nam_model = NAM(\n",
    "    config=config,\n",
    "    name='Testing_NAM',\n",
    "    num_inputs=len(nam_dataset[0][0]),\n",
    "    num_units=get_num_units(config, nam_dataset.features)\n",
    ")\n",
    "nam_model\n",
    "# NAM Training\n",
    "data_loaders = nam_dataset.train_dataloaders()\n",
    "for fold, (train_loader, val_loader) in enumerate(data_loaders):\n",
    "     tb_logger = TensorBoardLogger(\n",
    "          save_dir=config.logdir,\n",
    "          name=f'{nam_model.name}',\n",
    "          version=f'fold_{fold + 1}')\n",
    "\n",
    "     checkpoint_callback = ModelCheckpoint(\n",
    "          filename=tb_logger.log_dir + \"/{epoch:02d}-{val_loss:.4f}\",\n",
    "          monitor='val_loss',\n",
    "          save_top_k=config.save_top_k,\n",
    "          mode='min'\n",
    "     )\n",
    "     litmodel = LitNAM(config, nam_model)\n",
    "     pl.Trainer()\n",
    "     trainer = pl.Trainer(\n",
    "          logger=tb_logger,\n",
    "          max_epochs=config.num_epochs,\n",
    "          callbacks=checkpoint_callback,\n",
    "     )\n",
    "     trainer.fit(\n",
    "          litmodel,\n",
    "          train_dataloaders=train_loader,\n",
    "          val_dataloaders=val_loader)\n",
    "trainer.logger.experiment.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAM Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(litmodel, dataloaders=nam_dataset.test_dataloaders())\n",
    "fig = plot_mean_feature_importance(litmodel.model, nam_dataset)\n",
    "fig = plot_nams(litmodel.model, nam_dataset, num_cols= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_test = {'learning_rate': [0.001,0.005,0.01,0.03],\n",
    "              'interactions': [5,10,15],\n",
    "              'max_interaction_bins': [10,15,20],\n",
    "              'max_rounds': [5000,10000,15000,20000],\n",
    "              'min_samples_leaf': [2,3,5],\n",
    "              'max_leaves': [3,5,10]}\n",
    "n_HP_points_to_test=10\n",
    "LGBM_clf = LGBMClassifier(random_state=314, n_jobs=-1)\n",
    "LGBM_gs = RandomizedSearchCV(\n",
    "    estimator=LGBM_clf,\n",
    "    param_distributions=param_test,\n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=False,\n",
    ")\n",
    "LGBM_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for df_name, df in all_data.items():\n",
    "    display(df_name)\n",
    "    display(df.columns.to_list())\n",
    "    display('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_scores(clf, X_test, y_true, print_results=False):\n",
    "    scores = {}\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores['accuracy'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    scores['recall'] = recall_score(y_true=y_true, y_pred=y_pred)\n",
    "    scores['roc_auc'] = roc_auc_score(y_true=y_true, y_score=clf.predict_proba(X_test))\n",
    "\n",
    "    if print_results:\n",
    "        for score_name, score in scores.items():\n",
    "            print(f'{score_name}: {score}')\n",
    "\n",
    "    return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
